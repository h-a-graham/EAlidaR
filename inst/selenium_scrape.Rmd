---
title: "selenium_scraping"
output: html_document
---

## User input:
To run this script edit the arguments here...
```{r user_input, echo=TRUE, include=TRUE}

#conda env paths
conda.path <- 'C:/Users/hughg/Miniconda3/envs/R_python'

#conda env name
env.name <- 'R_python'

# Download Gecko Driver from here: https://github.com/mozilla/geckodriver/releases
gecko_exe <- 'C:/install_files/gecko/geckodriver-v0.27.0-win64/geckodriver.exe' 

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)

#set up python...
use_condaenv(condaenv =conda.path, env.name, required = TRUE)
# py_config()

wd <- dirname(getwd())

```




### Scrape Arc Web Map Object IDs.
This section of python uses selenium to upload zip files of 25km grids to the Defra Portal and then returns the object ID which is needed to construct the URL for that region.

```{python, echo=TRUE}
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import pandas as pd
from glob import glob
import time
import os
import pyautogui
import re
from pathlib import Path
from datetime import datetime
import warnings


def scrapestuff():
  link = 'https://environment.data.gov.uk/DefraDataDownload/?Mode=survey'
  
  work_dir = r.wd
  search_str = os.path.join(work_dir, 'data/grid_shp_zip/Tile_*.zip')
  zip_list = glob(search_str)
  zip_list = [str(Path(x)) for x in zip_list]
  # zip_list = zip_list[:2]
  
  # Set up firefox browser
  browser = webdriver.Firefox(executable_path = r.gecko_exe)
  browser.get(link)
  time.sleep(10)
  # print(browser.title)
  fail_list = []
  dump_list = []
  
  for file in zip_list:
    # get tile id number
    tile_n = int(re.search('Tile_(.*).zip', file).group(1))
    
    try:
      
      #click on upload button
      browser.find_element_by_css_selector('#buttonid').click()
      time.sleep(1)
      
      #send file to windows pop up
      pyautogui.write(file) 
      pyautogui.press('enter')
      time.sleep(5)
      
      # click 'get available tiles'
      browser.find_element_by_css_selector('.grid-item-container').click()
      time.sleep(39)
      
      #select DTM
      # browser.find_element_by_css_selector('#productSelect').click()
      
      select = Select(browser.find_element_by_css_selector('#productSelect'))
      select.select_by_visible_text('LIDAR Composite DTM')
      
        
      # get first link for download
      down_link = browser.find_element_by_css_selector('.data-ready-container > a:nth-child(1)').get_property('href')
      
      #retrieve arc object id from url
      result = re.search('interactive/(.*)/LIDARCOMP', down_link).group(1)
      
      #reset upload window
      browser.find_element_by_css_selector('div.result-options:nth-child(7) > input:nth-child(1)').click()
      time.sleep(0.5)
      
      # create pandas dataframe from results and append to list
      out_vals = [[tile_n, result]]
      scrape_out = pd.DataFrame(out_vals, columns=['tile_n', 'arc_code'])
      dump_list.append(scrape_out)
    
    except Exception as e:
      # Error handling - very general at the moment as I have no idea why errors will be thrown... Store tile number and Error in tuple and append to list...
      warnings.warn("Error has occurred for Tile {0}".format(tile_n))
      error_out = [tile_n, e]
      fail_list.append(error_out)
      
      pass
      
  
  # join list of pf dfs to single df
  combine_dfs = pd.concat(dump_list).reset_index(drop=True)
  
  browser.quit()
  return combine_dfs
  

if __name__ == '__main__':
  startTime = datetime.now()
  arc_id_df = scrapestuff()
  endTime = datetime.now() - startTime
  
  print('Python Script completed in {0}'.format(endTime))

```


Now we save the returned pandas dataframe as a csv file which will be used for constructing URLs in the package...

```{r save_dataframe, echo=TRUE}
# convert the pandas DF object to R DF and save as RDS
r_df <- reticulate::py$arc_id_df
save_path <- file.path(wd, 'data', 'arc_ids_25km.rds')
saveRDS(r_df, file = save_path)

#check output...
readRDS(save_path)

```


