---
title: "selenium_scraping"
output: html_document
---

## User input:
To run this script edit the arguments here...
```{r user_input, echo=TRUE, include=TRUE}

#conda env paths
conda.path <- 'C:/Users/hughg/Miniconda3/envs/R_python'

#conda env name
env.name <- 'R_python'

# Download Gecko Driver from here: https://github.com/mozilla/geckodriver/releases
gecko_exe <- 'C:/install_files/gecko/geckodriver-v0.27.0-win64/geckodriver.exe' 

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)

#set up python...
use_condaenv(condaenv =conda.path, env.name, required = TRUE)
# py_config()

wd <- dirname(getwd())

```




### Scrape Arc Web Map Object IDs.
This section of python uses selenium to upload zip files of 25km grids to the Defra Portal and then returns the object ID which is needed to construct the URL for that region.

```{python, echo=TRUE}
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import pyautogui
import pandas as pd
import math

from glob import glob
import time
import os
import re
from pathlib import Path
from datetime import datetime
import warnings


def scrapestuff():
  link = 'https://environment.data.gov.uk/DefraDataDownload/?Mode=survey'
  
  work_dir = r.wd
  search_str = os.path.join(work_dir, 'data/grid_shp_zip/Tile_*.zip')
  zip_list = glob(search_str)
  zip_list = [str(Path(x)) for x in zip_list]
  # zip_list = zip_list[:5]  # Use this line for testing/debugging
  
  # we must chunk up the list to avoid the limit of 10 uploads per session...
  zip_chunks = chunks(zip_list, 10)

  # print(browser.title)
  fail_list = []
  dump_list = []
  
  for chunk in zip_chunks:
    # Set up Firefox browser
    browser = webdriver.Firefox(executable_path = r.gecko_exe)
    # browser.implicitly_wait(60)
    browser.get(link)
    time.sleep(30)
    
    for file in chunk:
      # get tile id number
      tile_n = int(re.search('Tile_(.*).zip', file).group(1))
      # print(tile_n)
      try:
        
        #click on upload button
        browser.find_element_by_css_selector('#buttonid').click()
        time.sleep(1)
        
        #send file to windows pop up
        pyautogui.write(file) 
        time.sleep(3)
        pyautogui.press('enter')
        time.sleep(5)
        
        # click 'get available tiles'
        browser.find_element_by_css_selector('.grid-item-container').click()
        time.sleep(45)
        
        #select DTM
        browser.find_element_by_css_selector('#productSelect').click()
        
        select = Select(browser.find_element_by_css_selector('#productSelect'))
        select.select_by_visible_text('LIDAR Composite DTM')
        
          
        # get first link for download
        down_link = browser.find_element_by_css_selector('.data-ready-container > a:nth-child(1)').get_property('href')
        
        #retrieve arc object id from url
        result = re.search('interactive/(.*)/LIDARCOMP', down_link).group(1)
        
        #reset upload window
        browser.find_element_by_css_selector('div.result-options:nth-child(7) > input:nth-child(1)').click()
        # time.sleep(0.5)
        
        # create pandas dataframe from results and append to list
        out_vals = [[tile_n, result]]
        scrape_out = pd.DataFrame(out_vals, columns=['tile_n', 'arc_code'])
        dump_list.append(scrape_out)
      
      
    
      except Exception as e:
        # Error handling - very general at the moment as I have no idea why errors will be thrown... Store tile number and Error in tuple and append to list...
        warnings.warn("Error has occurred for Tile {0}".format(tile_n))
        error_out = [[tile_n, str(e)]]
        error_out_pd = pd.DataFrame(error_out, columns=['tile_n', 'error_message'])
        fail_list.append(error_out_pd)
        
        pass
        
    browser.quit()
  
  # join list of pf dfs to single df
  try:
    combine_dfs = pd.concat(dump_list).reset_index(drop=True)
  except Exception:
    warnings.warn('No data to join - somethings gone horribly wrong!!!')
    combine_dfs = []
  
  try:
    combine_errs = pd.concat(fail_list).reset_index(drop=True)
    warnings.warn('Errors have occurred - check Error log with py$error_df')
  except Exception:
    print('No Errors Occurred - YAY!!')
    combine_errs = []
  
  return combine_dfs, combine_errs

def chunks(l, n):
    n = max(1, n)
    return (l[i:i+n] for i in range(0, len(l), n))  

if __name__ == '__main__':
  startTime = datetime.now()
  arc_id_df, error_df = scrapestuff()
  endTime = datetime.now() - startTime
  
  print('Python Script completed in {0}'.format(endTime))

```


Now we save the returned pandas dataframe as a csv file which will be used for constructing URLs in the package...

```{r save_dataframe, echo=TRUE}
# convert the pandas DF object to R DF and save as RDS
r_df <- reticulate::py$arc_id_df
save_path <- file.path(wd, 'data', 'arc_ids_25km.rds')
saveRDS(r_df, file = save_path)

#check output...
readRDS(save_path)

```


